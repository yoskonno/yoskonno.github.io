{"componentChunkName":"component---src-templates-post-js","path":"/2019/06/10/aws-lambda上でwebスクレイピング/","result":{"data":{"wordpressPost":{"id":"b39c5c52-ebb3-5000-ad3c-d10380c97ab9","title":"AWS Lambda上でWebスクレイピング","excerpt":"<p>AWS Lambdaは様々なリソースの制限（例えば900秒を超える処理は実行できない等）があるため、スクレイピング処理をうまく動作させるために一苦労しました。これらの制限をクリアできる処理を動作させることに限定するという [&hellip;]</p>\n","slug":"aws-lambda%e4%b8%8a%e3%81%a7web%e3%82%b9%e3%82%af%e3%83%ac%e3%82%a4%e3%83%94%e3%83%b3%e3%82%b0","content":"\n<p>AWS Lambdaは様々なリソースの制限（例えば900秒を超える処理は実行できない等）があるため、スクレイピング処理をうまく動作させるために一苦労しました。これらの制限をクリアできる処理を動作させることに限定するという条件ではありますが、そこそこ使いやすいスクレイピング環境ができましたのでご紹介します。</p>\n\n\n\n<h2>デプロイ端末の環境構築</h2>\n\n\n\n<ul><li>Amazon Linux release 2 (Karoo)</li><li>serverless framework 1.30.3</li><li>Python 3.6</li></ul>\n\n\n\n<p>serverless frameworkについては以下のClassmethodさんの記事が参考になりますので、載せておきます。</p>\n\n\n\n<figure class=\"wp-block-embed-wordpress wp-block-embed is-type-wp-embed is-provider-developersio\"><div class=\"wp-block-embed__wrapper\">\nhttps://dev.classmethod.jp/cloud/serverless-framework-lambda-numpy-scipy/\n</div></figure>\n\n\n\n<p>上記の記事にしたがってserverlessのsampleというプロジェクトを作成したという前提で、このプロジェクトにスクレイピングに必要なパッケージ等を追加していきます。この時点でsampleプロジェクト配下は以下のようになっていることを確認します。</p>\n\n\n<pre class=\"brush: plain; title: ; notranslate\" title=\"\">\nsample/\n　├ handler.py\n　├ serverless.yml\n　├ package.json\n　├ package-lock.json\n　├ requirements.txt\n　├ node_modules/\n　├ .serverless/\n　├ .requirements.zip\n</pre>\n\n\n<h2>headless chromeとchrome driverをインストール</h2>\n\n\n\n<p>headlessのchromeはnpm、pipではインストールできないため、sample配下にbinディレクトリを作成して、その中で手動でダウンロードして展開します。</p>\n\n\n<pre class=\"brush: plain; title: ; notranslate\" title=\"\">\n(venv) $ cd sample\n(venv) $ mkdir bin\n(venv) $ cd bin\n\n(venv) $ curl -SL https://github.com/adieuadieu/serverless-chrome/releases/download/v1.0.0-37/stable-headless-chromium-amazonlinux-2017-03.zip &gt; headless-chromium.zip\n(venv) $ unzip headless-chromium.zip\n(venv) $ rm headless-chromium.zip\n\n(venv) $ curl -SL https://chromedriver.storage.googleapis.com/2.37/chromedriver_linux64.zip &gt; chromedriver.zip\n(venv) $ unzip chromedriver.zip\n(venv) $ rm chromedriver.zip\n</pre>\n\n\n<h2>スクレイピング用のパッケージをインストール</h2>\n\n\n\n<p>今回はselenium、beautifulSoup4、html5libを使用するので、これらをpipでインストールします。インストールした後は必ずrequirements.txtを更新します。</p>\n\n\n<pre class=\"brush: plain; title: ; notranslate\" title=\"\">\n(venv) $ cd sample\n(venv) $ pip install selenium\n(venv) $ pip install beautifulSoup4\n(venv) $ pip install html5lib\n\n(venv) $ pip freeze &gt; requirements.txt\n</pre>\n\n\n<h2>Pythonのスクレイピング処理を追加</h2>\n\n\n\n<p>既存のhandler.pyにスクレイピング処理を追加します。selenium、beautifulSoup4はWeb上にサンプルが多数ありますので、細かい部分は省略します。ここでは会員サイトである一覧表を表示し、その一覧をCSVファイルとしてS3にアップする処理を紹介します。</p>\n\n\n<pre class=\"brush: python; title: ; notranslate\" title=\"\">\ntry:\n    import unzip_requirements\nexcept ImportError:\n    pass\n \nimport sys\nimport os\nimport time\nimport calendar\nimport datetime\nimport logging\nimport subprocess\nimport boto3\nfrom pathlib import Path\nfrom bs4 import BeautifulSoup\nfrom selenium import webdriver\nfrom selenium.webdriver.common.desired_capabilities import DesiredCapabilities\nfrom selenium.webdriver.common.action_chains import ActionChains\nfrom selenium.webdriver.support.ui import Select\n \nmode2bucket = {\n    'dev': 'sample-scraping-dev-us-east-1',\n    'pro': 'sample-scraping'\n}\n  \ndef main(event, context):\n    print(&quot;main start&quot;) \n\n    # log level\n    logger = logging.getLogger()\n    logger.setLevel(logging.INFO)\n\n    STAGE = os.environ['selected_stage']\n    logger.info(&quot;stage[%s]&quot; % STAGE)\n\n    MODE = os.environ['scraping_mode']\n    logger.info(&quot;mode[%s]&quot; % MODE)\n\n    if context:\n        logger.info(&quot;start lambda_handler() [%s]&quot; % context.function_name)\n\n    #today\n    d = datetime.datetime.today()\n    #yesterday\n    d2 = d - datetime.timedelta(days=1)\n\n    target_url = 'https://target-url.jp/'\n    user_id = 'user1234@sample-email.jp'\n    password = 'xxxxxxxx'\n\n    try:\n        options = webdriver.ChromeOptions()\n        options.binary_location = &quot;./bin/headless-chromium&quot;\n        options.add_argument(&quot;--headless&quot;)\n        options.add_argument(&quot;--disable-gpu&quot;)\n        options.add_argument(&quot;--window-size=1280x1696&quot;)\n        options.add_argument(&quot;--disable-application-cache&quot;)\n        options.add_argument(&quot;--disable-infobars&quot;)\n        options.add_argument(&quot;--no-sandbox&quot;)\n        options.add_argument(&quot;--hide-scrollbars&quot;)\n        options.add_argument(&quot;--enable-logging&quot;)\n        options.add_argument(&quot;--log-level=0&quot;)\n        options.add_argument(&quot;--single-process&quot;)\n        options.add_argument(&quot;--ignore-certificate-errors&quot;)\n        options.add_argument(&quot;--homedir=/tmp&quot;)\n        driver = webdriver.Chrome(options=options, executable_path='./bin/chromedriver')\n        driver.implicitly_wait(30)\n        driver.get(target_url)\n\n        # login\n        logger.info(&quot;before log in&quot;)\n        driver.find_element_by_name(&quot;usr_id&quot;).send_keys(user_id)\n        driver.find_element_by_name(&quot;usr_password&quot;).send_keys(password)\n        driver.find_element_by_class_name(&quot;btnLogin&quot;).click()\n        logger.info(&quot;after log in&quot;)\n        logger.info(driver.current_url)\n\n        # click smartphone tab\n        element_to_hover_over = driver.find_element_by_id(&quot;navi02&quot;)\n        hover = ActionChains(driver).click(element_to_hover_over)\n        hover.perform()\n        driver.find_element_by_link_text('スマートフォン').click()\n        time.sleep(5)\n        logger.info(driver.current_url)\n\n        # check mode\n        end_date = &quot;&quot;\n        file_path = &quot;&quot;\n        if MODE == &quot;total&quot;:\n            #前の月の全日を取得\n            end_date = datetime.date(int(d.strftime(&quot;%Y&quot;)), int(d.strftime(&quot;%m&quot;)), 1) - datetime.timedelta(days=1)\n            file_path = 'sample_site/%s/%s/sample_total_%s.csv' % (d.strftime('%Y'), d.strftime('%m'), d.strftime('%Y%m%d'))\n        else:\n            end_date = d2\n            file_path = 'sample/%s/%s/sample_%s.csv' % (d2.strftime('%Y'), d2.strftime('%m'), d2.strftime('%Y%m%d'))\n\n        # click 'date range' radio button\n        driver.find_element_by_id(&quot;rep_send_date_radio&quot;).click()\n        logger.info(driver.current_url)\n\n        #input start date\n        start_date = end_date.strftime(&quot;%Y/%m/&quot;) + str(1).zfill(2)\n        logger.info(start_date)\n        logger.info(end_date.strftime(&quot;%Y/%m/%d&quot;))\n        driver.execute_script(&quot;document.getElementById('rep_start_date').value = '%s'&quot; % start_date)\n        driver.execute_script(&quot;document.getElementById('rep_end_date').value = '%s'&quot; % end_date.strftime(&quot;%Y/%m/%d&quot;))\n        logger.info(driver.current_url)\n\n        # click 'レポート表示' button\n        driver.find_element_by_id(&quot;rep_button_view&quot;).click()\n        logger.info(driver.current_url)\n        time.sleep(10)\n\n        html = driver.page_source\n        soup = BeautifulSoup(html, &quot;html5lib&quot;)\n\n        # create text in csv file\n        s3_text = ''\n\n        #add header\n        thead_tr_list = soup.find('table', id=&quot;rep_daily_all_option&quot;).findAll('thead')[0].findAll('tr')\n        for r1 in range(0, len(thead_tr_list)):\n            for elm1 in thead_tr_list[r1].findAll('th'):\n                th = elm1.text.strip()\n                s3_text += th\n                s3_text += '\\t'\n            s3_text += '\\n'\n        # add body\n        tbody_tr_list = soup.find_all('table', id=&quot;rep_daily_all_option&quot;)[0].findAll('tbody')[0].findAll('tr')\n        for r2 in range(0, len(tbody_tr_list)):\n            if (r2 % 100) == 0:\n                logger.info(&quot;line[%s]&quot; % str(r2))\n            for elm2 in tbody_tr_list[r2].findAll('td'):\n                td = elm2.text.strip()\n                s3_text += td\n                s3_text += '\\t'\n            s3_text += '\\n'\n        \n        driver.quit()\n\n        # put file to s3\n        s3 = boto3.resource('s3')\n        bucket = mode2bucket[STAGE]\n        s3Obj = s3.Object(bucket, file_path)\n        s3Obj.put(Body = bytes(s3_text, 'UTF-8'))\n\n        logger.info(&quot;finished&quot;)\n    except Exception as e:\n        # キャッチして例外をログに記録\n        logger.exception(e)\n        post_to_chat(&quot;例外が発生しました。&quot;, e)\n        return 1\n    return 0\n \nif __name__ == &quot;__main__&quot;:\n    main('', '')\n</pre>\n\n\n<h2>デプロイの準備</h2>\n\n\n\n<p>serverless.yml に上記で追加されたbinディレクトリの定義を追加します。（２２行目を追加しました。）</p>\n\n\n<pre class=\"brush: plain; title: ; notranslate\" title=\"\">\nservice: sample-scraping\n \nprovider:\n  name: aws\n  runtime: python3.6\n  stage: ${opt:stage, self:custom.defaultStage}\n\nplugins:\n  - serverless-python-requirements\n \ncustom:\n  defaultStage: dev\n  pythonRequirements:\n    dockerizePip: non-linux\n#    dockerFile: ./Dockerfile\n    slim: true\n    zip: true\n \npackage:\n  include:\n    - handler.py\n    - './bin/**'\n  exclude:\n    - '**'\n \nfunctions:\n  sample-main:\n    handler: sample.main\n    timeout: 900 # Lambda の最大が 900 秒\n    environment:\n      selected_stage: ${self:provider.stage}\n      scraping_mode: normal\n</pre>\n\n\n<h2>デプロイ・実行</h2>\n\n\n\n<p>下記のコマンドでデプロイと実行を行います。</p>\n\n\n<pre class=\"brush: plain; title: ; notranslate\" title=\"\">\n(venv) $ cd sample\n(venv) $ sls deploy\n(venv) $ sls invoke -f sample-main --log\n</pre>\n\n\n<h2>まとめ</h2>\n\n\n\n<p>serverlessは本当にお手軽に処理のデプロイができますし、Lambdaは使用した分だけの課金ですので、用途に合う要件であれば是非利用してみることをおすすめいたします。</p>\n","dateObject":"2019-06-10T00:00:25.000Z","date":"June 10, 2019","categories":[{"name":"Uncategorized","slug":"uncategorized"}],"tags":[{"name":"AWS","slug":"aws"},{"name":"Lambda","slug":"lambda"},{"name":"Python","slug":"python"},{"name":"scraping","slug":"scraping"},{"name":"Serverless","slug":"serverless"}],"author":{"name":"koji","slug":"koji"},"featured_media":{"media_details":{"sizes":{"large":{"source_url":"https://i0.wp.com/stg-engineering-wp.mobalab.net/wp-content/uploads/2019/06/Screen-Shot-2019-06-03-at-18.49.48.png?fit=1024%2C191&ssl=1","height":191,"width":1024},"medium_large":{"source_url":"https://i0.wp.com/stg-engineering-wp.mobalab.net/wp-content/uploads/2019/06/Screen-Shot-2019-06-03-at-18.49.48.png?fit=768%2C143&ssl=1","height":143,"width":768}}},"source_url":"https://stg-engineering-wp.mobalab.net/wp-content/uploads/2019/06/Screen-Shot-2019-06-03-at-18.49.48.png"},"wordpress_id":907}},"pageContext":{"id":"b39c5c52-ebb3-5000-ad3c-d10380c97ab9","nextPath":"/2019/05/31/validate-checksum-with-remote-file-using-md5sum/","nextTitle":"（小ネタ）md5sumを使ってリモートにあるファイルとのチェックサムを検証するワンライナー","prevPath":"/2019/06/17/trying-elasticsearch-apache-hadoop-integration/","prevTitle":"Elasticsearch for Apache Hadoopを使ってSparkからAmazon ESにデータと連携してみた"}}}