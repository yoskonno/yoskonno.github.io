{"componentChunkName":"component---src-templates-post-js","path":"/2019/01/10/develop-api-server-with-pre-trained-ml-model/","result":{"data":{"wordpressPost":{"id":"fcd4c293-3294-5b98-95cc-004cf190da36","title":"機械学習の学習済みモデルを使用して API サーバーを構築する","excerpt":"<p>やりたいこと 機械学習（ML）の学習済みのモデルを用いて、推論処理を行う API サーバーを構築する、というのが今回やりたいことです。 一般的に、学習には高機能な GPU インスタンスが必要なのに対して、推論処理は、CP [&hellip;]</p>\n","slug":"develop-api-server-with-pre-trained-ml-model","content":"\n<h2>やりたいこと</h2>\n\n\n\n<p>機械学習（ML）の学習済みのモデルを用いて、推論処理を行う API サーバーを構築する、というのが今回やりたいことです。</p>\n\n\n\n<p>一般的に、学習には高機能な GPU インスタンスが必要なのに対して、推論処理は、CPU インスタンス、あるいは小さめの GPU インスタンスでも問題無く動作します。従って、高性能インスタンスで学習したモデルを、比較的性能の低いインスタンスに移して動作させるというのが一般的です。</p>\n\n\n\n<p>今回のタスクは画像分類ですが、他の機械学習でもほぼ同じように出来ると思います。</p>\n\n\n\n<h2>環境</h2>\n\n\n\n<p>今回は以下の環境を用いました。</p>\n\n\n\n<ul><li><a rel=\"noreferrer noopener\" aria-label=\"AWS Deep Learning AMI (opens in a new tab)\" href=\"https://aws.amazon.com/machine-learning/amis/\" target=\"_blank\">AWS Deep Learning AMI</a> (Amazon Linux ベースのもの)を使用</li><li>ML フレームワークは Caffe</li><li><a rel=\"noreferrer noopener\" aria-label=\" (opens in a new tab)\" href=\"https://developer.nvidia.com/digits\" target=\"_blank\">NVIDIA DIGITS</a> 上で学習したモデルを使用</li><li>Web フレームワークには <a rel=\"noreferrer noopener\" aria-label=\" (opens in a new tab)\" href=\"http://flask.pocoo.org/\" target=\"_blank\">Flask</a></li><li>Web server は Apache + mod_wsgi</li></ul>\n\n\n\n<p>ただ、過去の自分にアドバイス出来るのであれば、以下の構成を使うかなと思います。理由に関しては後述します。</p>\n\n\n\n<ul><li>AWS Deep Learning AMI は使用しないで、使用する ML フレームワーク（今回は Caffe）のみがインストールされたインスタンスを使用する</li><li>Amazon Linux ベースでは無く Ubuntu ベースの AMI を使用し、Apache + mod_wsgi ではなく、Gunicorn などのスタンドアロンサーバーか、nginx + uWSGI などを使用する</li></ul>\n\n\n\n<!--more-->\n\n\n\n<h2>やったこと</h2>\n\n\n\n<h3>Flask で推論処理の API サーバーを実装</h3>\n\n\n\n<p>API サーバーの実装に、今回は Flask を使用しました。参考にしたのは以下のページです。</p>\n\n\n\n<p><a href=\"https://towardsdatascience.com/deploying-a-machine-learning-model-as-a-rest-api-4a03b865c166\" target=\"_blank\" rel=\"noreferrer noopener\" aria-label=\" (opens in a new tab)\">Deploying a Machine Learning Model as a REST API – Towards Data Science</a></p>\n\n\n\n<p>ただ、これも後から気づいたんですが、上のページのコードは若干冗長なんで、Flask のドキュメントも見ておくと良いと思います。</p>\n\n\n\n<p><a href=\"http://flask.pocoo.org/docs/1.0/quickstart/\" target=\"_blank\" rel=\"noreferrer noopener\" aria-label=\" (opens in a new tab)\">Quickstart — Flask 1.0.2 documentation</a></p>\n\n\n\n<p>出来たコードとしてはこんな感じです。本筋と関係ない部分で削られている箇所もありますので、ご注意下さい。なお、説明をコメントに記載してあります。</p>\n\n\n<pre class=\"brush: python; title: ; notranslate\" title=\"\">\n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n#\n# このファイルは api_server.py として保存\n\n# Flask 関連の import\nfrom flask import Flask\nfrom flask_restful import reqparse, abort, Api, Resource\n\n# Caffe の import\nimport caffe\n\n# その他 import\nimport os.path\nimport numpy as np\nimport sys\n\nimg_dir = '/path/to/image-dir'\nmodel_dir = '/path/to/model-dir'\nmodel_file = 'snapshot_iter.caffemodel'\n\n# AWS DL AMI に載ってる Caffe は、CPU モードが使えない\n# https://docs.aws.amazon.com/ja_jp/dlami/latest/devguide/cpu.html\ncaffe.set_mode_gpu()\n\n# モデルの読み込み\nnet = caffe.Net(model_dir + '/deploy.prototxt',\nmodel_dir + '/' + model_file,\ncaffe.TEST)\n\n# transformer の作成（詳細は Caffe のチュートリアルとかを参照）\ntransformer = caffe.io.Transformer({'data': net.blobs['data'].data.shape})\ntransformer.set_transpose('data', (2,0,1))\ntransformer.set_raw_scale('data', 255) # the reference model operates on images in [0,255] range instead of [0,1]\ntransformer.set_channel_swap('data', (2,1,0)) # the reference model has channels in BGR order instead of RGB\n\n# API の引数として、filename を受け取るように設定\nparser = reqparse.RequestParser()\nparser.add_argument('filename')\n\n# API のメイン処理\nclass ClassifyImage(Resource):\n    def get(self):\n        # ファイル名を取得\n        args = parser.parse_args()\n        filename = args['filename']\n        file_path = img_dir + '/' + filename\n\n        if not os.path.isfile(file_path):\n            return {'result': 'error', 'message': '{} not found'.format(file_path)}\n\n        # 入力データの設定\n        net.blobs['data'].data[...] = transformer.preprocess('data', caffe.io.load_image(file_path))\n        # 推論\n        out = net.forward()\n\n        # 推論結果のベクトル\n        a = out['softmax'][0]\n\n        # 結果の JSON object を生成\n        output = {'result': 'ok', 'prediction': a.argmax()}\n\n        return output\n\n# Flask の初期化みたいなの\napp = Flask(__name__)\napi = Api(app)\n\n# / にアクセスされたら、ClassifyImage.get を実行するように設定\n# 最近は以下のようにやるのが普通らしいが、今回は参考元のページに倣った\n# @app.route('/')\napi.add_resource(ClassifyImage, '/')\n\n# コマンドラインから実行されたときに限り、実行する\nif __name__ == '__main__':\n    app.run(debug=True)\n</pre>\n\n\n<p>ローカルでのテスト方法としては、以下の通りコマンドラインよりスタンドアロンのサーバーを起動できます。デフォルトでは localhost:5000 で LISTEN します。</p>\n\n\n<pre class=\"brush: plain; title: ; notranslate\" title=\"\">\n./api-server.py\n</pre>\n\n\n<p>後は、 <code>curl</code> コマンドなどでテストしてみて下さい。</p>\n\n\n<pre class=\"brush: plain; title: ; notranslate\" title=\"\">\ncurl http://localhost:5000/?filename=foo.jpg\n</pre>\n\n\n<p>ローカルのテストが完了したら、本番環境でも動作させたいところですが、デプロイの選択肢としてはいくつか考えられます。今回は mod_wsgi を選択しましたが、詳細は以下のページをご参照下さい。</p>\n\n\n\n<p><a href=\"http://flask.pocoo.org/docs/1.0/deploying/\" target=\"_blank\" rel=\"noreferrer noopener\" aria-label=\" (opens in a new tab)\">Deployment Options — Flask 1.0.2 documentation</a></p>\n\n\n\n<h3>mod_wsgi の設定</h3>\n\n\n\n<p>API サーバーが動作するようになったので、次はそれを mod_wsgi 経由で動かすようにします。</p>\n\n\n\n<h4>基本的な設定方法</h4>\n\n\n\n<p>最初に、以下のような *.wsgi ファイルを作成します。</p>\n\n\n<pre class=\"brush: python; title: ; notranslate\" title=\"\">\n# foo-api.wsgi\nimport sys, os\n\nimport logging\n# to output to Apache log\nlogging.basicConfig(stream = sys.stderr)\n\nsys.path.insert(0, os.path.abspath(os.path.dirname(__file__)))\n\nfrom api_server import app as application\n</pre>\n\n\n<p>Apache 関連ですが、最初は mod_wsgi をインストールします。</p>\n\n\n<pre class=\"brush: plain; title: ; notranslate\" title=\"\">\n# Amazon Linux 等、Red Hat 系の場合\nsudo yum install mod_wsgi-python27\n# Debian, Ubuntu 等\nsudo apt-get install libapache2-mod-wsgi\n</pre>\n\n\n<p>設定ファイルは以下のようになります。</p>\n\n\n<pre class=\"brush: plain; title: ; notranslate\" title=\"\">\nListen 5000\n\nWSGIDaemonProcess foo-api user=ec2-user group=apache threads=1\nWSGIScriptAlias / /var/www/foo-api/foo-api.wsgi\n\nWSGIProcessGroup foo-api\nWSGIApplicationGroup %{GLOBAL}\nOrder deny,allow\nAllow from all\n</pre>\n\n\n<p>あとは、Apache を再起動すれば、5000番ポートで API サーバーが待ち受けるようになります。</p>\n\n\n\n<h4>Anaconda 環境の Python を使用</h4>\n\n\n\n<p>ここまでの方法だと、システム環境の Python であれば問題無いんですが、AWS DL AMI の場合、各フレームワークの環境は Anaconda を使って切り替えて使用するようになっています。従って、もう一手間必要です。</p>\n\n\n\n<p>色々調べたんですが、</p>\n\n\n\n<ul><li><code>python-path</code> をいじれば大丈夫</li><li>mod_wsgi を手動でインストールしないとダメ</li></ul>\n\n\n\n<p>という2つの意見がありましたが、結論としては、（少なくとも自分の場合は）mod_wsgi を手動でインストールしないとだめでした。以下の Stack Overflow の質問も参考にしてみて下さい。</p>\n\n\n\n<ul><li><a rel=\"noreferrer noopener\" aria-label=\" (opens in a new tab)\" href=\"https://stackoverflow.com/questions/30581316/how-do-i-use-a-conda-environment-with-mod-wsgi\" target=\"_blank\">python &#8211; How do I use a conda environment with mod_wsgi? &#8211; Stack Overflow</a></li><li><a href=\"https://stackoverflow.com/questions/50605738/how-to-run-flask-apache-app-with-anaconda-python-version\" target=\"_blank\" rel=\"noreferrer noopener\" aria-label=\" (opens in a new tab)\">How to run Flask + Apache app with Anaconda Python version? &#8211; Stack Overflow</a></li></ul>\n\n\n\n<p>では、具体的な方法を説明します。</p>\n\n\n\n<p>まず、mod_wsgi などを pip でインストールし、パッケージで入れたものは削除します。</p>\n\n\n<pre class=\"brush: bash; title: ; notranslate\" title=\"\">\n# Caffe in Python 2.7 の環境に切り替える\nsource activate caffe_p27\n\n# flask はインストール済みっぽいので、flask-restful のみインストール\npip install flask-restful\n\n# 既存の mod_wsgi をアンインストール\nsudo yum remove mod_wsgi-python27\n\n# mod_wsgi のビルドに必要なのでインストール\nsudo yum install httpd-devel\n# mod_wsgi をインストール\npip install mod_wsgi\n</pre>\n\n\n<p>次に、mod_wsgi の Apache モジュールをインストールします。</p>\n\n\n<pre class=\"brush: bash; title: ; notranslate\" title=\"\">\n# root じゃないとインストールできませんが、caffe_p27 環境は\n# ec2-user にしか存在しないので\nsudo -i\nexport PATH=/home/ec2-user/anaconda3/bin/:$PATH\nsource activate caffe_p27\n\n# モジュールのインストール\nmod_wsgi-express install-module\nexit\n</pre>\n\n\n<p>Apache の設定ファイルを以下の通り修正します。</p>\n\n\n<pre class=\"brush: plain; title: ; notranslate\" title=\"\">\nListen 5000\n\nLoadModule wsgi_module &quot;/usr/lib64/httpd/modules/mod_wsgi-py27.so&quot;\nWSGIPythonHome &quot;/home/ec2-user/anaconda3/envs/caffe_p27&quot;\n\nWSGISocketPrefix /var/run/wsgi\n\nWSGIDaemonProcess foo-api user=ec2-user group=apache threads=1\nWSGIScriptAlias / /var/www/foo-api/foo-api.wsgi\n\nWSGIProcessGroup foo-api\nWSGIApplicationGroup %{GLOBAL}\nOrder deny,allow\nAllow from all\n</pre>\n\n\n<p>これに関しては、古いですが、以下のページが参考になりました。</p>\n\n\n\n<p><a rel=\"noreferrer noopener\" aria-label=\" (opens in a new tab)\" href=\"https://qiita.com/arc279/items/df28bd100cc2f72fad3c\" target=\"_blank\">virtualenv + flask + apache + wsgi で動かすまで &#8211; Qiita</a></p>\n\n\n\n<p>これで良さそうな気もしますが、この状態だと、*.so が見つからないというエラーも出ました。</p>\n\n\n\n<p>LD_LIBRARY_PATH を設定すれば良いのだろうと思ったのですが、 mod_wsgi の場合、新たにプロセスが作られるため、Apache のディレクティブ SetEnv で指定しても効きませんでした。結論としては、*.wsgi をいじって以下の行を入れました。</p>\n\n\n<pre class=\"brush: python; title: ; notranslate\" title=\"\">\nsys.path.insert(0, '/home/ec2-user/src/caffe_python_2/python')\n</pre>\n\n\n<p>参考にしたのは以下の gist です。</p>\n\n\n\n<p><a rel=\"noreferrer noopener\" aria-label=\" (opens in a new tab)\" href=\"https://gist.github.com/GrahamDumpleton/b380652b768e81a7f60c\" target=\"_blank\">Setting environment variables for Apache/mod_wsgi hosted Python application.</a></p>\n\n\n\n<h4>パス等の設定に関して補足</h4>\n\n\n\n<p>PYTHON_HOME, PYTHON_PATH の設定の仕方として、WSGIPythonHome, WSGIPythonPath あるいは WSGIDaemonProcess の python-home などいくつか方法があるようです。</p>\n\n\n\n<p>後方互換性とかその辺もあって色々ごちゃごちゃしているようなので、詳しくは以下のページを参考にして下さい。</p>\n\n\n\n<ul><li><a href=\"https://modwsgi.readthedocs.io/en/develop/configuration-directives/WSGIPythonPath.html\" target=\"_blank\" rel=\"noreferrer noopener\" aria-label=\" (opens in a new tab)\">WSGIPythonPath — mod_wsgi 4.6.5 documentation</a></li><li><a href=\"https://stackoverflow.com/questions/27832777/where-should-wsgipythonpath-point-in-my-virtualenv\">python &#8211; Where should WSGIPythonPath point in my virtualenv? &#8211; Stack Overflow</a></li></ul>\n\n\n\n<h2>他の選択肢</h2>\n\n\n\n<p>読んでもらったら分かる通り、今回やった方法はかなり面倒くさいと思いますので、ここでは他の選択肢を挙げます。</p>\n\n\n\n<h3>AWS DL AMI は使わない</h3>\n\n\n\n<p>一番面倒で時間がかかったのは、Anaconda の環境を mod_wsgi で動かす事でしたが、本番環境で複数のフレームワークを切り替えて使う必要はないため、システム環境の Python に直接 Caffe 等がインストールされている環境だとかなり楽を出来ると思います。</p>\n\n\n\n<h3>mod_wsgi を使わない</h3>\n\n\n\n<p>mod_wsgi はやっぱり色々面倒でしたので、以下のページに記載されている他の選択肢を使った方が簡単だったかもしれません。</p>\n\n\n\n<p><a rel=\"noreferrer noopener\" aria-label=\" (opens in a new tab)\" href=\"http://flask.pocoo.org/docs/1.0/deploying/\" target=\"_blank\">Deployment Options — Flask 1.0.2 documentation</a></p>\n\n\n\n<p>個人的には、次は uWSGI か Gunicorn を試してみようと思います。</p>\n\n\n\n<h3>Amazon Linux ベースの AMI を使わない</h3>\n\n\n\n<p>uWSGI や Gunicorn を使う際に、Amazon Linux ベースの AMI だとインストールが比較的面倒なので、色んなパッケージが充実している Ubuntu ベースの AMI の方が良いと思います。</p>\n\n\n\n<h3>Flask ではなく Bottle</h3>\n\n\n\n<p>知り合いに教えてもらったのですが、Python の軽量フレームワークで Bottle というのがあるそうで、こっちの方が Flask よりもさらに簡潔に見えます。次回はこっちを使ってみたいと思います。</p>\n\n\n\n<p><a href=\"https://bottlepy.org/docs/dev/\" target=\"_blank\" rel=\"noreferrer noopener\" aria-label=\" (opens in a new tab)\">Bottle: Python Web Framework — Bottle 0.13-dev documentation</a></p>\n\n\n\n<h2>まとめ</h2>\n\n\n\n<p>今回、紆余曲折を経ながらも、Caffe で学習済みのモデルを使用した、推論処理を行う API サーバーを構築することが出来ました。</p>\n\n\n\n<p>機械学習フレームワークは Python 製のものが大半なので、Flask あるいは Bottle の使い方を覚えておくと、簡単に API サーバーが構築できると思います。</p>\n","dateObject":"2019-01-10T07:40:32.000Z","date":"January 10, 2019","categories":[{"name":"Uncategorized","slug":"uncategorized"}],"tags":[{"name":"Caffe","slug":"caffe"},{"name":"Flask","slug":"flask"},{"name":"machine learning","slug":"machine-learning"},{"name":"mod_wsgi","slug":"mod_wsgi"},{"name":"Python","slug":"python"}],"author":{"name":"中の人（管理者）","slug":"engineering_8qmk0b"},"featured_media":{"media_details":{"sizes":{"large":null,"medium_large":{"source_url":"https://i2.wp.com/stg-engineering-wp.mobalab.net/wp-content/uploads/2019/01/server-1235959_800.jpg?fit=768%2C512&ssl=1","height":512,"width":768}}},"source_url":"https://stg-engineering-wp.mobalab.net/wp-content/uploads/2019/01/server-1235959_800.jpg"},"wordpress_id":483}},"pageContext":{"id":"fcd4c293-3294-5b98-95cc-004cf190da36","nextPath":"/2019/01/09/upload-splatoon2-battle-histories-regularly-on-lambda/","nextTitle":"AWS Lambdaを使ってスプラトゥーン2の戦績をstat.inkに定期保存できるようにした","prevPath":"/2019/01/16/react-nativeのライブラリをpatch-packageで手軽に修正する方法/","prevTitle":"React Nativeのライブラリをpatch-packageで手軽に修正する方法"}}}