{"componentChunkName":"component---src-templates-post-js","path":"/2020/09/23/静的データベースと動的データベース（spark-sqlの小ネ/","result":{"data":{"wordpressPost":{"id":"7ee6b83a-d7c9-5e95-b82b-8ce7d04ed671","title":"静的データベースと動的データベース（Spark SQLの小ネタ）","excerpt":"<p>このようなデータベースの種類を分ける概念は一般的にはないと思われますが、「Spark SQL」で開発しているとまさに動的だなぁという思いが湧いてくることが多々ありましたので記事にしてみました。（従来のRDBであるMySq [&hellip;]</p>\n","slug":"%e9%9d%99%e7%9a%84%e3%83%87%e3%83%bc%e3%82%bf%e3%83%99%e3%83%bc%e3%82%b9%e3%81%a8%e5%8b%95%e7%9a%84%e3%83%87%e3%83%bc%e3%82%bf%e3%83%99%e3%83%bc%e3%82%b9%ef%bc%88spark-sql%e3%81%ae%e5%b0%8f%e3%83%8d","content":"\n<p>このようなデータベースの種類を分ける概念は一般的にはないと思われますが、「Spark SQL」で開発しているとまさに動的だなぁという思いが湧いてくることが多々ありましたので記事にしてみました。（従来のRDBであるMySqlなどでも動的な使い方が出来るとは思いますが、スキーマを動的に変えたり、メモリ上で新しいテーブルを生成したりという使い方はあまりしないので、これらは静的という扱いにしています。）</p>\n\n\n\n<h2>Spark SQLの簡単な説明</h2>\n\n\n\n<p>Spark SQLは以下のような使い方を良くします。カラムをscalaプログラムで動的に追加して、別のテーブルとして保存するといった流れです。ちなみにテーブルはAWSのS3にparquetという形式のファイルとして保存できるので、それを読みこんでメモリ上にも新たにテーブルが作られるというイメージです。メモリ上のテーブルはSparkではDataFrameと呼ばれます。parquetファイルとDataFrameは両方ともスキーマ情報を含んでいますので、プログラムで特にその指定を明示的にしなくても扱えるので便利です。</p>\n\n\n<pre class=\"brush: scala; title: ; notranslate\" title=\"\">\n// S3のparquetファイルを読み込む\n// 1. カラム id, date, info を有するparquetファイルが\n//   「date=2020-09-14」ディレクトリの下に配置されている場合\n// 2. Sparkにはpartitionという機能があり、dateカラムの日付別に\n//   ディレクトリを持たせて管理することが出来ます\nval dataFrame = \n  spark.read.parquet(s&quot;s3://test-1234/sample_table/date=2020-09-14&quot;)\n\n// 新しいカラムを追加（column_newはカラム名、次の引数でカラムの値を指定）\nval dataFrameNew = \n  dataFrame.withColumn(&quot;column_new&quot;, lit(&quot;新しいカラム&quot;))\n\n// dataFrameNewを別のテーブルとして書き込み\ndataFrameNew\n  .select(&quot;id&quot;, &quot;date&quot;, &quot;info&quot;, &quot;column_new&quot;)\n  .write\n  .option(&quot;header&quot;,&quot;false&quot;).mode(&quot;overwrite&quot;).partitionBy(&quot;date&quot;)\n  .parquet(s&quot;s3://test-1234/new_table/date=2020-09-14&quot;)\n\n// dataFrameとdataFrameNewのスキーマを覗くとカラムが追加されたのが分かります\nprintln(dataFrame.schema.fieldNames)\nprintln(dataFrameNew.schema.fieldNames)\n</pre>\n\n\n<h2>スキーマ情報の違うテーブルを無理やり結合（SQLのunion相当）する</h2>\n\n\n\n<p>今回の記事のメインの部分になります。Sparkの動的な機能を使うとこんなことも出来るよ！という内容になってます。</p>\n\n\n\n<p>Sparkの２つのDataFrameを結合する場合、スキーマ情報を一致させ、さらにselectでカラムの順番を一致させるという手順が必要になります。これを実現するには、上記でも説明した通り、追加すべきカラムの名前を調べてからwithColumn()で明示的に追加する必要があるのですが、このステップをなんとか動的に出来ないかと試行錯誤したところ可能だということが分かりましたのご紹介します。</p>\n\n\n\n<p>この方法を使えばスキーマ情報の違うテーブルがいくつあろうと自動で結合してくれるので、テーブルを跨いで情報を調べたいときに便利です。</p>\n\n\n\n<p>仮に以下のような日付ごとにスキーマ情報の違うテーブルがs3に存在するとして説明をします。</p>\n\n\n<pre class=\"brush: plain; title: ; notranslate\" title=\"\">\n前提としてスキーマ情報は以下とします。\n  id　数値\n  date　文字列(yyyy-mm-dd)\n  info_a　文字列\n  info_b　文字列\n\n■テーブル１\ns3://test-1234/sample_table/date=2020-09-15/aaaa.parquet\n  =&gt; カラムは id, date, info_a を含む\n  +----+----------+------+\n  | id | date     |info_a|\n  +----+----------+------+\n  |   1|2020-09-15|   red|\n  |   2|2020-09-15|  blue|\n  +----+----------+------+\n\n■テーブル２\ns3://test-1234/sample_table/date=2020-09-16/bbbb.parquet\n  =&gt; カラムは id, date, info_b を含む\n  +----+----------+------+\n  | id | date     |info_b|\n  +----+----------+------+\n  |   1|2020-09-16| green|\n  |   2|2020-09-16|  gray|\n  +----+----------+------+\n\n\n上記の２つのテーブルを結合して以下のようなテーブルを生成したい\n+----+----------+------+------+                                                       \n| id | date     |info_a|info_b|\n+----+----------+------+------+\n|   1|2020-09-15|   red|  null|\n|   2|2020-09-15|  blue|  null|\n|   1|2020-09-16|  null| green|\n|   2|2020-09-16|  null|  gray|\n+----+----------+------+------+\n\n</pre>\n\n\n<p>上記のように同じテーブル内の別のpartitionに違うスキーマのparquetファイルを保存することは理論上可能です。</p>\n\n\n\n<p>info_a、info_bという名前を意識しなくても、自動で結合させるには以下のような方法を用います。</p>\n\n\n<pre class=\"brush: scala; title: ; notranslate\" title=\"\">\nval dateList = List(&quot;2020-09-15&quot;, &quot;2020-09-16&quot;)\n\n// date partitionごとにファイルを読み込んでDataFrameのリストを生成\nval dfListByDate = \n  // 日付のリストをDataFrameのリストに変換する\n  dateList.map { date =&gt; \n    spark.read\n      .parquet(s&quot;s3://test-1234/sample/date=$date&quot;)\n      // partitionごとにreadするとdateカラムが消えてしまうので付加しなおす\n      .withColumn(&quot;date&quot;, lit(s&quot;$date&quot;)) \n  }\n\n// 最終的に必要となるカラム名のリストを作成\n// 今回の例の場合、List(&quot;id&quot;, &quot;date&quot;, &quot;info_a&quot;, &quot;info_b&quot;)が得られる\nval mergedFieldNames = \n  dfListByDate.map(_.schema.fieldNames).flatten.toSet.toList\n\n// 最後にDataFrameのお互いに足りないカラムをNULL値で補い結合します。\n// ここでポイントとなるのはselect()する際に個別のカラム名ではなく\n// カラム名のリストを渡すことです。Spark SDKの素晴らしい柔軟性ですね。\nval result = \n  // カラムの追加後、reduceLeft()で全てのDataFrameを結合\n  dfListByDate.map { dfByDate =&gt; \n    // カラム名のリストのdiffを取れば不足しているカラム名が得られる\n    val neededfieldNames = \n      mergedFieldNames diff dfByDate.schema.fieldNames;\n\n    // 不足しているカラムがない場合はselect()して順番を合わせるだけ\n    if (neededfieldNames.length == 0) {\n      dfByDate.select(mergedFieldNames.map(col): _*) \n    // 不足しているカラムがある場合はwithColumn()で付加してからselect()する\n    } else { \n      neededfieldNames.foldLeft(dfByDate) {\n        (df: Dataset[Row], fieldName) =&gt; \n          df.withColumn(fieldName, lit(null)) \n      }.select(mergedFieldNames.map(col): _*) \n    } \n  }.reduceLeft(_ union _)\n\n// 結合完了！！！！\nresult.show\n</pre>\n\n\n<p>上記のdfListByDateの部分を自分の結合させたい好きなリストに変更してあげれば、どんなテーブルでも結合しほうだいになりました。</p>\n\n\n\n<p>ただし、idの値が１つは文字列、１つは数値のように型が違う場合はうまくいかないかもしれませんので、ご注意ください。この型の変換も上記の自動化の中に組み込めばさらに汎用性が高まりそうです。</p>\n\n\n\n<h2>まとめ</h2>\n\n\n\n<p>この便利さに触れると他のSQL（RDS, No SQL, SAPなど）の動的な仕組みというものがどうなっているかについて知りたくなってきますね。調べておもしろそうな内容あればまた記事にしたいと思います。</p>\n","dateObject":"2020-09-23T03:00:00.000Z","date":"September 23, 2020","categories":[{"name":"Uncategorized","slug":"uncategorized"}],"tags":[{"name":"database","slug":"database"},{"name":"Scala","slug":"scala"},{"name":"Spark","slug":"spark"},{"name":"sql","slug":"sql"}],"author":{"name":"koji","slug":"koji"},"featured_media":{"media_details":{"sizes":{"large":{"source_url":"https://i0.wp.com/stg-engineering-wp.mobalab.net/wp-content/uploads/2020/09/3892793_m.jpg?fit=1024%2C724&ssl=1","height":724,"width":1024},"medium_large":{"source_url":"https://i0.wp.com/stg-engineering-wp.mobalab.net/wp-content/uploads/2020/09/3892793_m.jpg?fit=768%2C543&ssl=1","height":543,"width":768}}},"source_url":"https://stg-engineering-wp.mobalab.net/wp-content/uploads/2020/09/3892793_m.jpg"},"wordpress_id":1649}},"pageContext":{"id":"7ee6b83a-d7c9-5e95-b82b-8ce7d04ed671","nextPath":"/2020/09/17/kedro触ってみた/","nextTitle":"kedro触ってみた","prevPath":"/2020/09/24/scala-error-handling-option-either/","prevTitle":"Scala の Option, Either とエラー処理"}}}