{"componentChunkName":"component---src-templates-post-js","path":"/2018/07/23/testing-spark-dataframe/","result":{"data":{"wordpressPost":{"id":"e81c4703-55ce-500d-90a9-7f888c2bf9fb","title":"Spark の DataFrame のテスト","excerpt":"<p>はじめに Apache Spark では、御存知の通り大規模なデータを高速に扱う事が出来ます。大規模データ処理のインフラという観点では、速度のチューニングのために、データ構造を調整したりデータ処理の順番を最適化したりとい [&hellip;]</p>\n","slug":"testing-spark-dataframe","content":"\n<h1>はじめに</h1>\n\n\n\n<p>Apache Spark では、御存知の通り大規模なデータを高速に扱う事が出来ます。大規模データ処理のインフラという観点では、速度のチューニングのために、データ構造を調整したりデータ処理の順番を最適化したりという作業は良くやると思います。</p>\n\n\n\n<p>一方、Spark を、与えられた入力データに対して所定の結果を出力するという、データ処理関数・プロシージャーと見た場合、入力データに対して正しい結果を出力する事を担保する必要があります。</p>\n\n\n\n<p>一般的なデータ処理のプログラムであれば、処理をテストしやすい形に分割した上で、ユニットテスト（単体テスト）を書くのが一般的だと思います。Spark の場合はどうすれば良いのでしょうか。</p>\n\n\n\n<p>本記事では、Apache Spark 上で動く、 <code>DataFrame</code> （あるいは <code>Dataset</code>） を使ったデータ処理プログラムの単体テストを書く方法について記載します。</p>\n\n\n\n<h1>前提条件・要件</h1>\n\n\n\n<p>今回扱うテスト対象のプログラムとしては、以下のような前提条件を想定します。</p>\n\n\n\n<ul><li>HDFS/S3 上の parquet ファイルを入力</li><li>DataFrame を使用してデータ処理を行う</li><li>リアルタイムではなくバッチ処理</li></ul>\n\n\n\n<p>テストの要件としては以下の通りです。</p>\n\n\n\n<ul><li>正しい入力データに対して、正しい結果が出ること担保する</li><li>エッジケースについてもテスト</li><li>入力データ異常、欠損データを早い段階で捕捉できることを担保する</li><li>テストはローカル環境で実施可能（外部の HDFS/S3 を必要としない）</li><li>sbt test で実行する</li></ul>\n\n\n\n<h1>やることの概要</h1>\n\n\n\n<p>今回、以下のライブラリを使います。<br><a href=\"https://github.com/holdenk/spark-testing-base\">holdenk/spark-testing-base: Base classes to use when writing tests with Spark</a></p>\n\n\n\n<p>テストケースは ScalaTest を使って書きますので、ScalaTest の経験が無い場合は、予めドキュメントに目を通しておくことをお勧めします。</p>\n\n\n\n<h1>実際の手順</h1>\n\n\n\n<h2>セットアップ</h2>\n\n\n\n<p>基本的に、README に書いてある通りです。</p>\n\n\n\n<p>spark-testing-base は、<code>&lt;Sparkのバージョン>_&lt;spark-testing-base自体のバージョン></code> のようにバージョンを指定します。<a href=\"https://github.com/holdenk/spark-testing-base/blob/master/RELEASE_NOTES.md\">Release Notes</a> を見ると、最新は 0.10.0 のようです。（私が使ったのは、もう少し古いバージョンです。）</p>\n\n\n\n<p>従って、以下のように dependency を build.sbt 等に追加します。</p>\n\n\n<pre class=\"brush: scala; title: ; notranslate\" title=\"\">\nval sparkVersion = &quot;2.3.1&quot;\nval sparkTestingBaseVersion = s&quot;${sparkVersion}_0.10.0&quot;\n\n&quot;com.holdenkarau&quot; %% &quot;spark-testing-base&quot; % sparkTestingBaseVersion % &quot;test&quot;,\n</pre>\n\n\n<p>また、デフォルトのメモリ設定だと <code>OutOfMemoryError</code> が発生しやすいので、以下の設定も追加します。</p>\n\n\n<pre class=\"brush: plain; title: ; notranslate\" title=\"\">\nfork in Test := true\njavaOptions ++= Seq(&quot;-Xms512M&quot;, &quot;-Xmx2048M&quot;, &quot;-XX:MaxPermSize=2048M&quot;, &quot;-XX:+CMSClassUnloadingEnabled&quot;),\n</pre>\n\n\n<h2>テスト対象のクラス</h2>\n\n\n\n<p>テスト対象のクラスを書く際には、通常の Scala と同様のプラクティスが当てはまることが多いです。具体的には</p>\n\n\n\n<ul><li>副作用を局所化する</li><li>適切に分割する</li></ul>\n\n\n\n<p>などです。</p>\n\n\n\n<p>また、今回はタイトルの通り <code>DataFrame</code> (= <code>Dataset[Row]</code>)をテストするのですが、例として以下のようなクラスを考えます。ここでは、<code>foo</code> メソッドをテストすると仮定します。</p>\n\n\n<pre class=\"brush: scala; title: ; notranslate\" title=\"\">\nobject FooMain {\n  def main(args: Array[String]): Unit = {\n    implicit val session: SparkSession = SparkSession\n      .builder()\n      .appName(&quot;example-app&quot;)\n      .enableHiveSupport()\n      .getOrCreate()\n\n    val df1 = session\n      .read\n      .parquet(s&quot;s3://example-bucket/data1&quot;)\n\n    val df2 = session\n      .read\n      .parquet(s&quot;s3://example-bucket/data2&quot;)\n\n    val resultDf = foo(df1, df2)\n\n    resultDf.write.parquet(s&quot;s3://example-bucket/result&quot;)\n  }\n\n  def foo(\n    inputDf1: Dataset[Row],\n    inputDf2: Dataset[Row]\n  )(implicit spark: SparkSession): Dataset[Row] {\n    import spark.implicits._\n\n    inputDf1.join(inputDf2, &quot;id&quot;)\n      .filter(....)\n      .select(....)\n    // など\n  }\n}\n</pre>\n\n\n<h2>テストの書き方</h2>\n\n\n\n<p>次に、テストの書き方ですが、まずはコードから載せます。</p>\n\n\n<pre class=\"brush: scala; title: ; notranslate\" title=\"\">\nimport com.holdenkarau.spark.testing.DataFrameSuiteBase\nimport org.apache.spark.sql.{Dataset, Row}\nimport org.scalatest.FunSuite\n\nclass FooTest extends FunSuite with DataFrameSuiteBase {\n  test(&quot;test something&quot;) {\n    // ここの `spark` は、DataFrameSuiteBase の親である\n    // DataFrameSuiteBaseLike で定義されている\n    // その他、`sc` (SparkContext) なども使用可能\n    import spark.implicits._\n\n    val df1 = List(\n      (1, 5, &quot;abc&quot;),\n      (2, 11, &quot;def&quot;),\n      (3, 1, &quot;Z&quot;)\n    ).toDF(&quot;id&quot;, &quot;col_1a&quot;, &quot;col_1b&quot;).alias(&quot;df1&quot;)\n    val df2 = List(\n      (2, 5),\n      (3, 3),\n      (4, 1)\n    ).toDF(&quot;id&quot;, &quot;col_2&quot;).alias(&quot;df2&quot;)\n\n    val resultDf = FooMain.foo(df1, df2)(spark)\n    val expectedDf = List(\n      (2, 16),\n      (3, 4)\n    ).toDF(&quot;id&quot;, &quot;sum&quot;)\n\n    assertDataFrameEquals(expectedDf, resultDf)\n  }\n}\n</pre>\n\n\n<p>コードで大体分かるかと思いますが、</p>\n\n\n\n<ul><li>テスト対象メソッドに渡す <code>DataFrame</code> は、 <code>List</code> などから作成</li><li>期待される結果の <code>DataFrame</code> も、同様に <code>List</code> などから作成</li><li>実際の結果との比較には、spark-testing-base が提供する assertion を使用</li></ul>\n\n\n\n<p>といった流れです。</p>\n\n\n\n<h2>実行</h2>\n\n\n\n<p><code>sbt test</code> で実行します。詳細は ScalaTest のドキュメントを参照して下さい。</p>\n\n\n\n<h1>その他、細かい情報</h1>\n\n\n\n<h2>Hive support</h2>\n\n\n\n<p>Spark には、Hive support という機能があり、それを使うと Hive のテーブルを読み書きしたりできます。（詳細は Spark のドキュメントを参照。）</p>\n\n\n\n<p>ローカルのテストを実行する環境によっては、この機能が有効となっていると、以下のようなエラーが発生する可能性があります。</p>\n\n\n<pre class=\"brush: plain; title: ; notranslate\" title=\"\">\njava.lang.IllegalArgumentException: Error while instantiating 'org.apache.spark.sql.hive.HiveSessionState':\n(略)\nCause: java.lang.RuntimeException: java.lang.RuntimeException: The root scratch dir: /tmp/hive on HDFS should be writable. Current permissions are: rwxr-xr-x\n\n</pre>\n\n\n<p>原因としては、以下の Stack Overflow にある通り色々なケースがあるようです。</p>\n\n\n\n<p><a href=\"https://stackoverflow.com/questions/42942620/spark-2-1-error-while-instantiating-hivesessionstate\">Spark 2.1 &#8211; Error While instantiating HiveSessionState &#8211; Stack Overflow</a></p>\n\n\n\n<p>ただ、テストコードの中で Hive support の機能を使う必要が無ければ、これを無効にすることが出来ます。</p>\n\n\n\n<p>方法としては <code>DataFrameSuiteBaseLike#enableHiveSupport</code> を override して false を返すようにすれば良いです。具体的には以下のようにします。</p>\n\n\n<pre class=\"brush: scala; title: ; notranslate\" title=\"\">\nclass FooTest extends FunSuite with DataFrameSuiteBase {\n  override protected implicit def enableHiveSupport: Boolean = false\n\n  test(&quot;test foo&quot;) {\n    // :\n    // :\n  }\n}\n</pre>\n\n\n<h2>その他のクラスの使い方は Wiki を</h2>\n\n\n\n<p>上の例では <code>DataFrameSuiteBase</code> を使いましたが、それと同様に <code>DatasetSuiteBase</code> というのもあります。また、stream を対象とした <code>StreamingSuiteBase</code> もあります。</p>\n\n\n\n<p>また、上の例のように List などからテストデータを作らず、簡単に作成してくれる <code>XxxGenerator</code> というのもありますので、テストの種類に応じて仕様を検討してみて下さい。</p>\n\n\n\n<p>詳しくは公式の wiki を参照。</p>\n\n\n\n<p><a href=\"https://github.com/holdenk/spark-testing-base/wiki\">Home · holdenk/spark-testing-base Wiki</a></p>\n\n\n\n<h1>まとめ</h1>\n\n\n\n<p>spark-testing-base を使うと、Spark のデータ処理に対するテストが比較的簡単に書けます。</p>\n\n\n\n<p>Spark では大規模なデータを扱うことが多いので、本番稼働が始まってからデータの不備やエッジケースなどで予期せぬ挙動やシステム停止が起きると、小規模なシステムに比べて影響が大きいです。テストを書くことで、そうしたリスクを減らすことができます。</p>\n","dateObject":"2018-07-23T01:59:55.000Z","date":"July 23, 2018","categories":[{"name":"Uncategorized","slug":"uncategorized"}],"tags":[{"name":"Spark","slug":"spark"}],"author":{"name":"中の人（管理者）","slug":"engineering_8qmk0b"},"featured_media":{"media_details":{"sizes":{"large":null,"medium_large":{"source_url":"https://i0.wp.com/stg-engineering-wp.mobalab.net/wp-content/uploads/2018/07/hook-1425312_1000.jpg?fit=768%2C461&ssl=1","height":461,"width":768}}},"source_url":"https://stg-engineering-wp.mobalab.net/wp-content/uploads/2018/07/hook-1425312_1000.jpg"},"wordpress_id":122}},"pageContext":{"id":"e81c4703-55ce-500d-90a9-7f888c2bf9fb","nextPath":"/2018/07/18/how-to-check-the-public-ip-on-cli/","nextTitle":"How to check the public (outbound) IP on CLI","prevPath":"/2018/07/25/article-1how-to-manage-aws-lambda-functions-with-serverless-framework/","prevTitle":"Article 1:How to manage AWS Lambda Functions with Serverless Framework"}}}