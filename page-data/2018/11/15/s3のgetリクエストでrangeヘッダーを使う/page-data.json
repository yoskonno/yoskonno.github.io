{"componentChunkName":"component---src-templates-post-js","path":"/2018/11/15/s3のgetリクエストでrangeヘッダーを使う/","result":{"data":{"wordpressPost":{"id":"6434a5b7-1f14-5002-95b3-880806952038","title":"S3のGETリクエストでRangeヘッダーを使う","excerpt":"<p>久しぶりの投稿です。 先日とあるプロジェクトで、Amazon S3に保存されているCSVファイルをデータベースにインポートする機能をPHPで作りました。 PHPではCSVを fopen と言う関数を使う事で、ストリームか [&hellip;]</p>\n","slug":"s3%e3%81%aeget%e3%83%aa%e3%82%af%e3%82%a8%e3%82%b9%e3%83%88%e3%81%a7range%e3%83%98%e3%83%83%e3%83%80%e3%83%bc%e3%82%92%e4%bd%bf%e3%81%86","content":"<p class=\"part\" data-startline=\"3\" data-endline=\"3\">久しぶりの投稿です。</p>\n<p class=\"part\" data-startline=\"5\" data-endline=\"6\">先日とあるプロジェクトで、Amazon S3に保存されているCSVファイルをデータベースにインポートする機能をPHPで作りました。<br />\nPHPではCSVを <a href=\"http://php.net/manual/ja/function.fgetcsv.php\" target=\"_blank\" rel=\"noopener\">fopen</a> と言う関数を使う事で、ストリームからCSVの行を1行ずつ配列として受け取る事ができます。</p>\n<p class=\"part\" data-startline=\"8\" data-endline=\"9\">さて、今回要件として、CSVファイルは最大数GBまで許容されると言う事と、ジョブは並列で動作するのでローカルディスクにこのデータ全体を一旦DLするのはNG <sup class=\"footnote-ref\"><a id=\"fnref1\" href=\"#fn1\">[1]</a></sup> と言う条件がありました。<br />\nローカルディスク上に保存されているファイルであれば <a href=\"http://php.net/manual/ja/function.fopen.php\" target=\"_blank\" rel=\"noopener\">fopen</a> で開いたストリームを使って問題なく fgetcsv が処理できますが、このような事情の為、何か良い方法が無いか調べた所、S3のGETリクエストは  <a href=\"https://tools.ietf.org/html/rfc2616#section-14.35\" target=\"_blank\" rel=\"noopener\">Range (RFC 2616)</a> が使える事が分かりました。</p>\n<p class=\"part\" data-startline=\"11\" data-endline=\"12\">これは、HTTPのリクエストヘッダーに <code>Content-Range: bytes 0-100000</code> の様に指定すると、 先頭の100KBまでのチャンクをダウンロードできると言う物です。<br />\nこれを使う事で、部分的にCSVのデータを取得してそれをパースしていけば、メモリもディスクスペースも使わずにインポートが実現できます。</p>\n<p class=\"part\" data-startline=\"14\" data-endline=\"14\">因みに、Content-Rangeに指定する値のフォーマットは色々あるので詳しくは <a href=\"https://tools.ietf.org/html/rfc2616#section-14.35\" target=\"_blank\" rel=\"noopener\">RFC 2616</a> を参照の事。</p>\n<h2 id=\"実際に使ってみる\" class=\"part\" data-startline=\"16\" data-endline=\"16\">実際に使ってみる</h2>\n<p class=\"part\" data-startline=\"18\" data-endline=\"19\">今回はPHPで書いているので、<code>aws-sdk-php</code> を使います。<br />\n実はContent−Rangeの指定には特殊な処理は必要なく、単純に GetObject のoptions に<code>Range</code> として渡せばOKです。</p>\n<pre class=\"part\" data-startline=\"21\" data-endline=\"25\"><code class=\"php hljs\">$object = $s3-&gt;getObject([<span class=\"hljs-string\">'Bucket'</span> =&gt; <span class=\"hljs-string\">'test'</span>, <span class=\"hljs-string\">'Key'</span> =&gt; <span class=\"hljs-string\">'awesome.csv'</span>, <span class=\"hljs-string\">'Range'</span> =&gt; sprintf(<span class=\"hljs-string\">'bytes=%s-%s'</span>, <span class=\"hljs-number\">0</span>, <span class=\"hljs-number\">1024</span> * <span class=\"hljs-number\">1024</span> * <span class=\"hljs-number\">10</span>)]));\n\n<span class=\"hljs-keyword\">echo</span> $object-&gt;__toString(); <span class=\"hljs-comment\">// CSVの最初の10MBを取得</span>\n</code></pre>\n<p class=\"part\" data-startline=\"27\" data-endline=\"27\">後は、チャンクごとにCSVを読み込んでいく訳ですが、これにはちょっとした下処理が必要で、1つのチャンクで完全なCSV行が読み取れない場合があるので、その場合はそのチャンクをバッファリングしておいて次のチャンクと連結して再度パースする必要がある為です。</p>\n<p class=\"part\" data-startline=\"29\" data-endline=\"30\">詳細は長くなるので割愛しますが、<a href=\"https://github.com/issei-m/s3-get-with-range-php\" target=\"_blank\" rel=\"noopener\">GitHub</a>にサンプルコードを用意したので興味のある方は見てみて下さい。<br />\nそこでは最後の3行でCSV行をループで検証していますが、1GB超の巨大なCSVファイルであってもメモリは12MB前後しか消費されず、検証は成功と言えます。</p>\n<pre class=\"part\" data-startline=\"32\" data-endline=\"36\"><code class=\"php hljs\">foreach ($csvRows as $v) {\n    echo \\sprintf('%s, %s / %.4f MB', $v[0], $v[4], \\memory_get_usage(true) / 1024 / 1024), \"\\n\";\n}\n</code></pre>\n<hr class=\"footnotes-sep\" />\n<section class=\"footnotes\">\n<ol class=\"footnotes-list\">\n<li id=\"fn1\" class=\"footnote-item\"><a href=\"https://github.com/aws/aws-sdk-php\" target=\"_blank\" rel=\"noopener\">AWS SDK for PHP</a>のS3 <code>GetObject</code> のBodyはPSR-7のStreamで表現されていますが、<a href=\"https://github.com/guzzle/psr7/blob/23e7d3812a82bdbff4a5cb8e4628bcc06eec5d68/src/functions.php#L81\" target=\"_blank\" rel=\"noopener\">実態は <code>php://temp</code></a> なので、ファイルサイズがでかいとディスク容量を圧迫します <a class=\"footnote-backref\" href=\"#fnref1\">↩︎</a></li>\n</ol>\n</section>\n","dateObject":"2018-11-15T02:45:34.000Z","date":"November 15, 2018","categories":[{"name":"Uncategorized","slug":"uncategorized"}],"tags":[{"name":"AWS","slug":"aws"},{"name":"HTTP","slug":"http"},{"name":"PHP","slug":"php"},{"name":"Range","slug":"range"},{"name":"RFC-2616","slug":"rfc-2616"},{"name":"S3","slug":"s3"}],"author":{"name":"issei_m","slug":"issei"},"featured_media":{"media_details":{"sizes":{"large":null,"medium_large":null}},"source_url":"https://stg-engineering-wp.mobalab.net/wp-content/uploads/2018/11/aws-s3-eyecatch.png"},"wordpress_id":427}},"pageContext":{"id":"6434a5b7-1f14-5002-95b3-880806952038","nextPath":"/2018/10/09/licensed-scrum-master-になりました/","nextTitle":"Licensed Scrum Master になりました","prevPath":"/2018/12/07/phpのコレクションライブラリ-knapsack-を使ってみる/","prevTitle":"PHPのコレクションライブラリ Knapsack を使ってみる"}}}